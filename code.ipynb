{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcad9f5cf6fa4a09af9e7ef454d91b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--distilbert--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f15527e04b4e7db9fab72117412d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54f558579a444b69ff784e88d085bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df26ba3d67914c05af8726bd5903112a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63970c06809f423db5966777f495b7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adcd7e659b74915a687694f7da1ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b01c663595f4121ad93423c5ba049e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the importance of soil moisture in farming.\n",
      "The most important factor for determining whether a plant is good or bad, as well as its ability to withstand pests and pathogens can be found on soils that are too wet to hold up against them.\n"
     ]
    }
   ],
   "source": [
    "#output from distilegpt2 (without wiki)\n",
    "import torch\n",
    "import wikipediaapi\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id to eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Function to generate text in a chat bot style\n",
    "def generate_chatbot_response(prompt, max_length=200, temperature=0.5, top_k=30, top_p=0.85, repetition_penalty=1.1):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "    \n",
    "    # Generate text with controlled sampling\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the importance of soil moisture in farming.\"\n",
    "response = generate_chatbot_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki\n",
      "Gaganyaan Gaganyaan ([ɡəɡənəjɑːnə]; from Sanskrit: gagana, \"celestial\" and yāna, \"craft, vehicle\") is an Indian crewed orbital spacecraft intended to be the formative spacecraft of the Indian Human Spaceflight Programme. The spacecraft is being designed to carry three people, and a planned upgraded version will be equipped with rendezvous and docking capabilities. In its maiden crewed mission, the Indian Space Research Organisation (ISRO)'s largely autonomous 5.3-metric ton capsule will orbit the Earth at 400 km altitude for up to seven days with a two- or three-person crew on board. The first crewed mission was originally planned to be launched on ISRO's HLVM3 rocket in December 2021. As of October 2023, it is expected to be launched by 2025.\n",
      "The Hindustan Aeronautics Limited (HAL)-manufactured crew module underwent its first uncrewed experimental flight on December 18, 2014. As of May 2019, design of the crew module has been completed. Defence Research and Development Organisation (DRDO) will provide support for critical human-centric systems and technologies such as space-grade food, crew healthcare, radiation measurement and protection, parachutes for the safe recovery of the crew module, and the fire suppression system.\n",
      "On June 11, 2020, it was announced that the first uncrewed Gaganyaan launch would be delayed due to the COVID-19 pandemic in India. The overall timeline for crewed launches was expected to remain unaffected. ISRO chairman S. Somanath announced in 2022 that the first crewed mission would not take place until 2024 at the earliest because of safety concerns.\n",
      "The Gaganyaan Mission will be led by V. R. Lalithambika, the former Director of the Directorate of the Human Spaceflight Programme with ISRO Chairman S Somnath and S. Unnikrishnan Nair, Director of Vikram Sarabhai Space Centre. Imtiaz Ali Khan superseded V. R. Lalithambika as the Director of the Directorate of Human Spaceflight Programme.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:544: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import wikipediaapi\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "\n",
    "# Move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id to eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Function to generate text in a chatbot style\n",
    "def generate_chatbot_response(prompt, max_length=1000, temperature=0.5, top_k=30, top_p=0.85, repetition_penalty=1.1):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "    \n",
    "    # Generate text with controlled sampling\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Function to search Wikipedia for relevant information\n",
    "def search_wikipedia(query, lang='en'):\n",
    "    # Specify a valid user agent with an email address\n",
    "    user_agent = \"isro_immersion2024/1.0 (atharvagarole678@gmail.com)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(lang, headers={'User-Agent': user_agent})\n",
    "    \n",
    "    page = wiki_wiki.page(query)\n",
    "    if page.exists() and len(page.summary) > 100:  # Assuming a summary is relevant if it's longer than 100 characters\n",
    "        return page.summary\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Gaganyaan\"\n",
    "wikipedia_info = search_wikipedia(prompt)\n",
    "\n",
    "if wikipedia_info:\n",
    "    # If Wikipedia has relevant info, rephrase it\n",
    "    print(\"wiki\")\n",
    "    response = generate_chatbot_response(f\"{prompt} {wikipedia_info}\")\n",
    "else:\n",
    "    # Otherwise, generate a response based on the prompt alone\n",
    "    response = generate_chatbot_response(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaganyaan Gaganyaan ([ɡəɡənəjɑːnə]; from Sanskrit: gagana, \"celestial\" and yāna, \"craft, vehicle\") is an Indian crewed orbital spacecraft intended to be the formative spacecraft of the Indian Human Spaceflight Programme. The spacecraft is being designed to carry three people, and a planned upgraded version will be equipped with rendezvous and docking capabilities. In its maiden crewed mission, the Indian Space Research Organisation (ISRO)'s largely autonomous 5.3-metric ton capsule will orbit the Earth at 400 km altitude for up to seven days with a two- or three-person crew on board. The first crewed mission was originally planned to be launched on ISRO's HLVM3 rocket in December 2021. As of October 2023, it is expected to be launched by 2025.\n",
      "The Hindustan Aeronautics Limited (HAL)-manufactured crew module underwent its first uncrewed experimental flight on December 18, 2014. As of May 2019, design of the crew module has been completed. Defence Research and Development Organisation (DRDO) will provide support for critical human-centric systems and technologies such as space-grade food, crew healthcare, radiation measurement and protection, parachutes for the safe recovery of the crew module, and the fire suppression system.\n",
      "On June 11, 2020, it was announced that the first uncrewed Gaganyaan launch would be delayed due to the COVID-19 pandemic in India. The overall timeline for crewed launches was expected to remain unaffected. ISRO chairman S. Somanath announced in 2022 that the first crewed mission would not take place until 2024 at the earliest because of safety concerns.\n",
      "The Gaganyaan Mission will be led by V. R. Lalithambika, the former Director of the Directorate of the Human Spaceflight Programme with ISRO Chairman S Somnath and S. Unnikrishnan Nair, Director of Vikram Sarabhai Space Centre. Imtiaz Ali Khan superseded V. R. Lalithambika as the Director of the Directorate of Human Spaceflight Programme.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForTokenClassification, pipeline\n",
    "import wikipediaapi\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "\n",
    "# Load the NER model and pipeline\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Move the NER model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ner_model.to(device)\n",
    "\n",
    "# Create NER pipeline with the device\n",
    "ner_pipeline = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\", device=device)\n",
    "\n",
    "# Move the language model to GPU\n",
    "model.to(device)\n",
    "\n",
    "# Set pad_token_id to eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Function to generate text in a chatbot style\n",
    "def generate_chatbot_response(prompt, max_length=1000, temperature=0.5, top_k=30, top_p=0.85, repetition_penalty=1.1):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id).long().to(device)\n",
    "    \n",
    "    # Generate text with controlled sampling\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Function to search Wikipedia for relevant information\n",
    "def search_wikipedia(query, lang='en'):\n",
    "    # Specify a valid user agent with an email address\n",
    "    user_agent = \"isro_immersion2024/1.0 (atharvagarole678@gmail.com)\"\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(lang, headers={'User-Agent': user_agent})\n",
    "    \n",
    "    page = wiki_wiki.page(query)\n",
    "    if page.exists() and len(page.summary) > 100:  # Assuming a summary is relevant if it's longer than 100 characters\n",
    "        return page.summary\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to extract keywords using NER\n",
    "def extract_keywords(prompt):\n",
    "    ner_results = ner_pipeline(prompt)\n",
    "    keywords = [result['word'] for result in ner_results if result['entity_group'] in [\"ORG\", \"LOC\", \"MISC\"]]\n",
    "    return \" \".join(keywords)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"tell me about Gaganyaan\"\n",
    "keywords = extract_keywords(prompt)\n",
    "wikipedia_info = search_wikipedia(keywords)\n",
    "\n",
    "if wikipedia_info:\n",
    "    # If Wikipedia has relevant info, rephrase it\n",
    "    response = generate_chatbot_response(f\"{prompt} {wikipedia_info}\")\n",
    "else:\n",
    "    # Otherwise, generate a response based on the prompt alone\n",
    "    response = generate_chatbot_response(prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think it's the one that says I'm not a spy.\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
